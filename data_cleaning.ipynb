{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mental health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list=[\"text\",\"class\"]\n",
    "mh_data=pd.read_csv(r\"Suicide_Detection_dataset1.csv\",usecols=col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mh_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mh_data.shape)\n",
    "print(type(mh_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(mh_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_data=mh_data.dropna()\n",
    "np.where(pd.isnull(mh_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(232086):\n",
    "    if(mh_data.iloc[i][1]!='suicide' and mh_data.iloc[i][1]!='non-suicide'):\n",
    "        print(i,end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suicide_Detection2=mh_data\n",
    "#Suicide_Detection2.to_csv('Suicide_Detection2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md=pd.read_csv(\"Suicide_Detection2.csv\",usecols=[\"text\",\"class\"],encoding='latin1')\n",
    "md.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(232027):\n",
    "    if(md.iloc[i][1]!='suicide' and md.iloc[i][1]!='non-suicide'):\n",
    "        print(i,end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Suicide_Dataset2.csv' to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232027, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md=pd.read_csv(\"Suicide_Detection2.csv\",usecols=[\"text\",\"class\"],encoding='latin1')\n",
    "md.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'md' variable is loaded with dataset\n",
    "type(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(md[\"text\"][0]))\n",
    "print(type(md[\"class\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232027, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pd.isnull(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(232027):\n",
    "    if(md.iloc[i][1]!='suicide' and md.iloc[i][1]!='non-suicide'):\n",
    "        print(i,end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    Text Normalizer class normalizes text\n",
    "    \"\"\"\n",
    "    def __init__(self ):\n",
    "        self.stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "        self.wnl=nltk.stem.WordNetLemmatizer()\n",
    "        self.ps=nltk.stem.PorterStemmer()\n",
    "        self.wn=nltk.corpus.wordnet\n",
    "    \n",
    "    def normalize(self, text,clean=True,rm_stopwords=True,rm_special_chars=True,expand_conts=True,caseConvert=True,lemmatize=True,stem=True):\n",
    "        if clean:\n",
    "            text=self.cleanText(text)\n",
    "        if expand_conts:\n",
    "            text=self.expandContractions(text)\n",
    "        if lemmatize:\n",
    "            text=self.lemmatizeText(text)\n",
    "        if stem:\n",
    "            text=self.stemText(text)\n",
    "        if rm_stopwords:\n",
    "            text=self.removeStopwords(text)\n",
    "        if rm_special_chars:\n",
    "            text=self.removeSpecialChars(text)\n",
    "        if caseConvert:\n",
    "            text=self.caseConvert(text,True)\n",
    "        else:\n",
    "            text=self.caseConvert(text,False)\n",
    "        return text\n",
    "        \n",
    "    def caseConvert(self, text, upper=True):\n",
    "        if upper:\n",
    "            return text.upper()\n",
    "        return text.lower()\n",
    "    \n",
    "    def cleanText(self, page):\n",
    "        pass\n",
    "    \n",
    "    def tokenizeText(self,text):\n",
    "        words=nltk.word_tokenize(text)\n",
    "        tokens=[word.strip() for word in words]\n",
    "        return tokens\n",
    "    \n",
    "    def removeStopwords(self, text):\n",
    "        tokens=self.tokenizeText(text)\n",
    "        filter_tokens=[token for token in tokens if token not in self.stopwords]\n",
    "        text=' '.join(filter_tokens)\n",
    "        return text\n",
    "    \n",
    "    def removeSpecialChars(self,text):\n",
    "        tokens=self.tokenizeText(text)\n",
    "        pattern=re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "        filtered_tokens=filter(None,[pattern.sub('',token) for token in tokens])        \n",
    "        filtered_text=' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "    \n",
    "    def expandContractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    def pos_tagger(self,text):\n",
    "        tag_map={\n",
    "            'J':wn.ADJ,\n",
    "            'V':wn.VERB,\n",
    "            'N':wn.NOUN,\n",
    "            'R':wn.ADV\n",
    "        }\n",
    "        text=self.tokentizeText(text)\n",
    "        tagged_text=nltk.pos_tag(text)\n",
    "        wn_tagged_text=[(word.lower(), tag_map[pos_tag[0]]) for word, pos_tag in tagged_text]\n",
    "        return wn_tagged_text\n",
    "    \n",
    "    def lemmatizeText(self, text):\n",
    "        tagged_text=self.pos_tagger(text)\n",
    "        #[(word,pos_tag)]\n",
    "        lemmatized_tokens=[self.wnl.lemmatize(word,pos_tag) if pos_tag else word for word, pos_tag in tagged_text ]\n",
    "        lemmatized_text=' '.join(lemmatized_tokens)\n",
    "        return lemmatized_text\n",
    "    \n",
    "    def stemText(self, text):\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m      4\u001b[0m tn\u001b[38;5;241m=\u001b[39mTextNormalizer()\n\u001b[1;32m----> 5\u001b[0m normalized_text\u001b[38;5;241m=\u001b[39m\u001b[43mtn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe quick brown fox... couldn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mve ,jump over the lazy dog?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalized_text)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mTextNormalizer.normalize\u001b[1;34m(self, text, clean, rm_stopwords, rm_special_chars, expand_conts, caseConvert, lemmatize, stem)\u001b[0m\n\u001b[0;32m     13\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleanText(text)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expand_conts:\n\u001b[1;32m---> 15\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpandContractions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lemmatize:\n\u001b[0;32m     17\u001b[0m     text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlemmatizeText(text)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mTextNormalizer.expandContractions\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpandContractions\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontractions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\contractions\\__init__.py:102\u001b[0m, in \u001b[0;36mfix\u001b[1;34m(s, leftovers, slang)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfix\u001b[39m(s, leftovers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, slang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    101\u001b[0m     ts \u001b[38;5;241m=\u001b[39m replacers[(leftovers, slang)]\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textsearch\\__init__.py:553\u001b[0m, in \u001b[0;36mTextSearch.replace\u001b[1;34m(self, text, return_entities)\u001b[0m\n\u001b[0;32m    551\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[0;32m    552\u001b[0m current_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 553\u001b[0m _text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_case_in_search \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m    554\u001b[0m handlers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers \u001b[38;5;241m+\u001b[39m [(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbounds_check)]\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m end_index, (length, norm) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomaton\u001b[38;5;241m.\u001b[39miter(_text):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download()\n",
    "\n",
    "#tn=TextNormalizer()\n",
    "#normalized_text=tn.normalize(\"The quick brown fox... couldn't've ,jump over the lazy dog?\")\n",
    "#print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "wnl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SUDHIR\n",
      "[nltk_data]     KUTRE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#printing stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "port_stem=PorterStemmer()\n",
    "def stemming(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('\\[.*?\\]', '',text)\n",
    "    text=re.sub(\"\\\\W\",\" \",text)\n",
    "    text=re.sub('https?://\\S+|www\\.\\S+','',text)\n",
    "    text=re.sub('<.*?>+','',text)\n",
    "    text=re.sub('[%s]'% re.escape(string.punctuation),'',text)\n",
    "    text=re.sub('\\n','',text)\n",
    "    text=re.sub('\\w*\\d\\w*','',text)\n",
    "    text=text.split()\n",
    "    text=[port_stem.stem(word) for word in text if not word in stopwords.words('english')]\n",
    "    text=' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
